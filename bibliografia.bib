% bibliografia
%@book{womak:effective-java,
%    author = {J. Bloch},
%    title = {Effective Java},
%    publisher = {Pearson},
%    year = {2009}
%}

@misc{cifar10_paper,
    author = {Doon, Raveen and Kumar Rawat, Tarun and Gautam, Shweta},
    booktitle = {2018 IEEE Punecon},
    title = {Cifar-10 Classification using Deep Convolutional Neural Network},
    year = {2018},
    volume = {},
    number = {},
    pages = {1-5},
    doi = {10.1109/PUNECON.2018.8745428} }

@misc{fashion_mnist_paper,
    doi = {10.48550/ARXIV.1708.07747},
    url = {https://arxiv.org/abs/1708.07747},
    author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
    title = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
    year = {2017},
}

@INPROCEEDINGS{imagenet_paper,
    author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
    booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
    title = {ImageNet: A large-scale hierarchical image database},
    year = {2009},
    pages = {248-255},
    doi = {10.1109/CVPR.2009.5206848}
}

@misc{ms_coco_paper,
    doi = {10.48550/ARXIV.1405.0312},
    url = {https://arxiv.org/abs/1405.0312},
    author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll√°r, Piotr},
    title = {Microsoft COCO: Common Objects in Context},
    publisher = {arXiv},
    year = {2014},
}

@INPROCEEDINGS{Geiger2012CVPR,
    author = {Andreas Geiger and Philip Lenz and Raquel Urtasun},
    title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},
    booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2012}
}

@article{alex_net_paper,
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
    title = {ImageNet Classification with Deep Convolutional Neural Networks},
    url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
    volume = {25},
    year = {2012}
}

@article{vgg_paper,
    author = {Simonyan, Karen and Zisserman, Andrew},
    title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
    url = {https://arxiv.org/abs/1409.1556},
    year = {2014},
}

@article{inception_v1_paper,
    author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
    title = {Going Deeper with Convolutions},
    url = {https://arxiv.org/abs/1409.4842},
    year = {2014},
}

@article{inception_v2_paper,
    author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
    title = {Rethinking the Inception Architecture for Computer Vision},
    url = {https://arxiv.org/abs/1512.00567},
    year = {2015},
}

@article{resnet_paper,
    doi = {10.48550/ARXIV.1512.03385},
    url = {https://arxiv.org/abs/1512.03385},
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Deep Residual Learning for Image Recognition},
    year = {2015},
}


@misc{vit_paper,
    doi = {10.48550/ARXIV.2010.11929},
    url = {https://arxiv.org/abs/2010.11929},
    author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
    title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    year = {2020},
}

@misc{transformer_paper,
    doi = {10.48550/ARXIV.1706.03762},
    url = {https://arxiv.org/abs/1706.03762},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
    title = {Attention Is All You Need},
    year = {2017},
}

@INPROCEEDINGS{kitti_dataset,
    author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
    booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
    title = {Are we ready for autonomous driving? The KITTI vision benchmark suite},
    year = {2012},
    volume = {},
    number = {},
    pages = {3354-3361},
    abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
    keywords = {},
    doi = {10.1109/CVPR.2012.6248074},
    ISSN = {1063-6919},
    month = {June}, }

@article{vo_state_of_art,
    author = {Alkendi, Yusra and Seneviratne, Lakmal and Zweiri, Yahya},
    year = {2021},
    month = {05},
    pages = {1-1},
    title = {State of the Art in Vision-Based Localization Techniques for Autonomous Navigation Systems},
    volume = {PP},
    journal = {IEEE Access},
    doi = {10.1109/ACCESS.2021.3082778}
}
%
%@online{site:owasp,
%    title = {Owasp},
%    url = {https://owasp.org/www-project-mobile-top-10/},
%}

@site{site:hebbian-plasticity,
    year = {1949},
    title = {Hebbian Plasticity},
    url = {https://en.wikipedia.org/wiki/Hebbian_theory},
}

@misc{yolov3_paper,
    doi = {10.48550/ARXIV.1804.02767},
    url = {https://arxiv.org/abs/1804.02767},
    author = {Redmon, Joseph and Farhadi, Ali},
    title = {YOLOv3: An Incremental Improvement},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@site{transformer_in_cv,
    year = {},
    title = {Transformer in Computer Vision},
    url = {https://www.axelera.ai/vision-transformers-in-computer-vision/},
}
@misc{measuring_robustness_of_visual_slam,
    doi = {10.48550/ARXIV.1910.04755},

    url = {https://arxiv.org/abs/1910.04755},

    author = {Prokhorov, David and Zhukov, Dmitry and Barinova, Olga and Vorontsova, Anna and Konushin, Anton},

    keywords = {Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {Measuring robustness of Visual SLAM},

    publisher = {arXiv},

    year = {2019},

    copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{horn_method,
    author = {Horn, Berthold},
    year = {1987},
    month = {04},
    pages = {629-642},
    title = {Closed-Form Solution of Absolute Orientation Using Unit Quaternions},
    volume = {4},
    journal = {Journal of the Optical Society A},
    doi = {10.1364/JOSAA.4.000629}
}