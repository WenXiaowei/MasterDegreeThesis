\section{Losses}\label{sec:losses}
Loss function is an important component of the training process, it is used to evaluate the performance of the model, and by optimizing it (minimizing it or maximizing it) we can improve the performance of the model.
In our case, we tried different loss functions:
\begin{itemize}
    \item Mean Squared Error (MSE);
    \item Weighted MSE;
\end{itemize}

\subsection[Mean Square Error]{Mean Square Error (MSE)}\label{subsec:mean-square-error-(mse)}
MSE is the most common loss function used in regression problems.
Given the ground truth pose $p_{gt}$ and the predicted pose $p_{pred}$, the MSE is defined as:
\begin{equation}
    \label{eq:mean-square-error}
    MSE = \frac{1}{N} \sum_{i=1}^{N} (pose_{gt} - pose_{pred})^2
\end{equation}

This loss function is derived from the square of Euclidean distance, it is always positive value that decreases as the error approaches zero.

\subsection[Weighted Mean Square Error]{Weighted Mean Square Error-WMSE}\label{subsec:weighted-mean-square-error-wmse}
WMSE is a variation of the MSE, which is useful when we want to give more importance to some components of the pose.
For example, given a sequence of poses: $p_0$, $p_1$ \dots $p_n$, we want to give more importance to the poses more distant from the origin, because we want the poses far away from the origin to be closer to the ground truth.
In this way, we impose the network to give more importance for the poses far away from the origin as the errors accumulate over steps.
The formula of the WMSE is:
\begin{equation}
    \label{eq:weighted-mean-square-error}
    WMSE = \frac{1}{N} \sum_{i=1}^{N} i * (pose_{gt} - pose_{pred})^2
\end{equation}
In the equation, we chose $i$ as the weight of the pose, but we can use any other function of $i$.
