\section{Models}\label{sec:models}
%**************************************************************
\subsection{Standard model}\label{subsec:standard-model}
We used PyTorch deep-learning framework to implement the models because it's flexible, intuitive, and easy to use.
We mainly used ResNet18 and ResNet50 as \textbf{feature extractor}, we used the pre-trained version of these models, which are trained on ImageNet dataset by the authors of the library.

We took the model, removed the last layer, in this way, as output of the layer we obtain a tensor of size 512 or 2048, depending on the model, and we used this as the embedding.

Then, we used the \textit{TransformerEncoderLayer}, \textit{TransformerEncoder}, \textit{TransformerDecoderLayer}, and \textit{TransformerDecoder} from the PyTorch library.
Where TransformerEncoder is single layer, TransformerEncoderLayer is the container for the encoder layers, and it takes as input the number of layers.
Meanwhile, the TransformerEncoder needs as input:
\begin{itemize}
    \item \textit{embedding dimension}: 512 or 2048, depending on the feature extractor;
    \item \textit{n\_head}: the number of attention heads;
    \item \textit{mlp\_dim}: the number of neurons in the MLP;
    \item \textit{drop\_out}: the dropout rate;
\end{itemize}
The TransformerDecoder and TransformerDecoderLayer are similar to the TransformerEncoder and TransformerEncoderLayer, respectively.
The difference between TransformerEncoderLayer and the TransformerDecoderLayer is that the latter in \textit{forward} method, it takes as mandatory parameter the target sequence, while the former doesn't.
There are also some optional parameter, like the \textit{src\_key\_padding\_mask} and \textit{src\_key\_padding\_mask}, which functionality is explained in \hyperref[subsec:transformer]{\S2.1.2}.

Finally, depending on different version of the model we introduced in \hyperref[subsec:models]{\S4.1}, we used different MLP to predict the pose.

\subsection{Auto-regressive Model}\label{subsec:auto-regressive-model}
% todo write this section