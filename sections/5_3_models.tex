\section{Models}\label{sec:models}
%**************************************************************

\subsection{Standard model}\label{subsec:standard-model}
We used PyTorch deep-learning framework to implement the models because it's flexible, intuitive, and easy to use.
We mainly used ResNet18 and ResNet50 as \textbf{feature extractor}, we used the pre-trained version of these models, which are trained on ImageNet dataset by the authors of the library.

We took the model, removed the last layer, in this way, as output of the layer we obtain a tensor of size 512 or 2048, depending on the model, and we used this as the embedding.

Then, we used the \textit{TransformerEncoderLayer}, \textit{TransformerEncoder}, \textit{TransformerDecoderLayer}, and \textit{TransformerDecoder} from the PyTorch library.
Where TransformerEncoder is single layer, TransformerEncoderLayer is the container for the encoder layers, and it takes as input the number of layers.
Meanwhile, the TransformerEncoder needs as input:
\begin{itemize}
    \item \textit{embedding dimension}: 512 or 2048, depending on the feature extractor;
    \item \textit{n\_head}: the number of attention heads;
    \item \textit{mlp\_dim}: the number of neurons in the MLP;
    \item \textit{drop\_out}: the dropout rate;
\end{itemize}
The output of the Encoder is known as memory.
The TransformerDecoder and TransformerDecoderLayer are similar to the TransformerEncoder and TransformerEncoderLayer, respectively.
The difference between TransformerEncoderLayer and the TransformerDecoderLayer is that the latter in \textit{forward} method, it takes as mandatory parameter the target sequence, while the former doesn't.
There are also some optional parameter, like the \textit{src\_key\_padding\_mask} and \textit{src\_key\_padding\_mask}, which functionality is explained in \hyperref[subsec:transformer]{\S2.1.2 Theoretical foundations - Transformer}.

Finally, depending on different version of the model we introduced in \hyperref[subsec:models]{\S4.1 Experiments - Models}, we used different MLP to predict the pose.

\subsection{Auto-regressive model}\label{subsec:auto-regressive-model}
What we want to do is to predict the future poses of the sequence, given the past poses.
In details, we want to predict the pose at time $t_i$, given the poses at time $t_0, t_1, \dots, t_{-i-1}$.
To do this, we should treat the training time and the inference time differently.

In the training phase, we feed the decoder with the memory and the target, which is obtained by combining the ground-truth pose embedding, and the learnable vector (with the same shape of a sequence), the combination is defined as follows:
\begin{lstlisting}[captionpos=b, label={lst:lst-training-target}, caption={Training target}, language=Python]
def _training_target(self, gt):
    """
    :return: the target for the training phase, that should be:
    [
    [GT, trg, trg, trg, trg],
    [GT, GT,  trg, trg, trg],
    [Gt, GT,  GT,  trg,  trg],
    [Gt, GT,  GT,  GT,  trg]
    ]
    """
    learnable_target = repeat(self.learnable_target, 'n d -> b n d', b=gt.shape[0])
    lower_triangular = torch.tril(gt)
    upper_triangular = torch.triu(learnable_target, 1)
    return lower_triangular + upper_triangular
\end{lstlisting}
In the example, from code snippet 5.3, each row is a sequence of poses, and each element of the row is a pose.
\textit{GT} is the ground-truth pose embedding computed by the pose auto-encoder, meanwhile the \textit{trg} is part of the learnable vector.
In the example, the batch\_size is 4 and seq\_len is 5.
With this target, at the time 0, we have the \textit{GT} at first pose, also because it's considered the origin of that sequence, then what we have to predict is the second pose.
For the second sequence, we have \textit{GT} at pose 0 and 1, and we have to predict the pose 2.
And so on so forth.
Last thing we should pay attention to is that the loss should be computed only for the elements on the first diagonal of the matrix, because in the lower triangular matrix, we have the ground truth, and we don't care about the predictions of the upper triangular matrix.

At inference time, the target for the decoder is composed by it-self's output at previous time step combined with the learnable vector as the following function:
\begin{lstlisting}[captionpos=b, label={lst:lst-inference-target}, caption={Inference target}, language=Python]
def _inference_target(self, predictions, column_index):
    """
    column_index is the index of the column that we want to predict: always 0 < x < seq_len
    :return: the target for the inference phase, that should be:
    for column_index = 1:
    [
    [GT, trg, trg, trg, trg],
    ...
    [GT, trg, trg, trg, trg],
    ]
    for column_index = 2:
    [
    [GT, GT, trg, trg, trg],
    ...
    [GT, GT, trg, trg, trg],
    ]
    """
    # origin_embed shape: batch_size, 1, emb_size,
    # learnable_target shape: batch_size, seq_len, emb_size
    # predictions shape: batch_size, seq_len, emb_size
    return torch.cat((predictions[:column_index, :], self.learnable_target[column_index:, :]), dim=0)
\end{lstlisting}

The code for inference is the following the one shown in \textbf{Code 5.5}.
\begin{lstlisting}[captionpos=b, label={lst:lst-inference}, caption={Inference}, language=Python]
predictions = torch.zeros_like(x)
predictions[:, 0, :] = gt_target  # embedding of the origin
for i in range(1, x.shape[0]):
    new_target = self._inference_target(predictions[i], i).unsqueeze(0)
    predicted = self.decoder(new_target, x[i].unsqueeze(0), tgt_mask=generate_upper_triangular_mask(x.shape).to(self.device))
    predictions[i, :, :] = predicted[:, :, :]
    if i < x.shape[0] - 1:
        predictions[i + 1, :, :] = predicted[:, :, :]
\end{lstlisting}
In the previous code snippet, the \textit{gt\_target} origin embedding, we instantiate a tensor with the same shape of the input, we set the first column to the origin embedding, and then we iterate over the sequence.
At each iteration, we predict the pose at $i$-th, given the previously predicted poses at time $t_0, t_1, \dots, t_{i-1}$ for all the sequences of the batch.
Differently from the training, we should compute the loss for the whole \textit{predicted} tensor, because we are predicting all the poses.
