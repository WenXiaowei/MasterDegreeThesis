\section{Why Transformer?}\label{sec:why-transformer}

We think that the transformer is a good candidate to solve the problem of visual odometry because it is able to learn the sequence of images and the sequence of poses in a self-supervised way.

Although, the transformer contrasts strongly with CNNs.
Because in CNNs the features are statically weighted using pretrained weights, while in the transformer the features are dynamically weighted based on the context and receptive fields of individual network layers are typically local and limited by the convolutional kernel size.

The success of the CNN derives from the fact the shared weights explicitly encode how specific identical patterns are repeated in images, this ensures the convergence also in relatively small dataset, but also limits the modelling capacity.
Meanwhile, the Vision Transformers do not enforce such strict bias.
But in the same time, transformer has the higher learning capacity, but it's harder to train.

So, given the high learning capacity of the transformer, and the capability to adapt to various tasks also because the transformer is a general purpose architecture, we think that the transformer is a good candidate to solve the problem of visual odometry.


